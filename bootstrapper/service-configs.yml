# GenAI Vanilla Stack - Complete Service Configuration Matrix
# This file defines all services and their SOURCE-based variations

# Services with SOURCE-based configuration variations
# These services can run in different modes based on SOURCE environment variables
source_configurable:
  
  llm_provider:
    # Containerized CPU-only Ollama
    ollama-container-cpu:
      scale: 1
      environment:
        OLLAMA_ENDPOINT: "http://ollama:11434"
      deploy: {}
      extra_hosts: []
      
    # Containerized GPU-accelerated Ollama  
    ollama-container-gpu:
      scale: 1
      environment:
        OLLAMA_ENDPOINT: "http://ollama:11434"
        NVIDIA_VISIBLE_DEVICES: "all"
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                capabilities: [gpu]
      extra_hosts: []
      
    # Local Ollama running on host machine
    ollama-localhost:
      scale: 0
      environment:
        OLLAMA_ENDPOINT: "http://host.docker.internal:11434"
      deploy: {}
      extra_hosts:
        - "host.docker.internal:host-gateway"
        
    # External Ollama service
    ollama-external:
      scale: 0
      environment:
        OLLAMA_ENDPOINT: "${LLM_PROVIDER_EXTERNAL_URL}"
      deploy: {}
      extra_hosts: []
      
    # API-based Ollama service
    api:
      scale: 0
      environment:
        OLLAMA_ENDPOINT: "http://api.provider.com"  # Configure API endpoint
      deploy: {}
      extra_hosts: []
    
    # Disabled - no LLM provider
    disabled:
      scale: 0
      environment:
        OLLAMA_ENDPOINT: "http://ollama:11434"  # Default for dependent services
      deploy: {}
      extra_hosts: []

  comfyui:
    # Containerized CPU-only ComfyUI
    container-cpu:
      scale: 1
      environment:
        COMFYUI_ARGS: "--listen --cpu"
        COMFYUI_ENDPOINT: "http://comfyui:18188"
        AUTO_UPDATE: "false"
        WEB_ENABLE_AUTH: "false"
        ENABLE_QUICKTUNNEL: "false"
        SERVERLESS: "false"
        IS_LOCAL_COMFYUI: "false"
        COMFYUI_LOCAL_MODELS_PATH: "./empty"
      deploy: {}
      extra_hosts: []
      
    # Containerized GPU-accelerated ComfyUI
    container-gpu:
      scale: 1
      environment:
        COMFYUI_ARGS: "--listen --force-fp16"
        COMFYUI_ENDPOINT: "http://comfyui:18188"
        AUTO_UPDATE: "true"
        NVIDIA_VISIBLE_DEVICES: "all"
        NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
        WEB_ENABLE_AUTH: "false"
        ENABLE_QUICKTUNNEL: "false"
        SERVERLESS: "false"
        IS_LOCAL_COMFYUI: "false"
        COMFYUI_LOCAL_MODELS_PATH: "./empty"
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: 1
                capabilities: [gpu]
          limits:
            cpus: "${PROD_ENV_COMFYUI_CPUS:-2}"
            memory: "${PROD_ENV_COMFYUI_MEM_LIMIT:-4g}"
      extra_hosts: []
      
    # Local ComfyUI running on host machine
    localhost:
      scale: 0
      environment:
        COMFYUI_ENDPOINT: "http://host.docker.internal:8000"
        IS_LOCAL_COMFYUI: "true"
        COMFYUI_LOCAL_MODELS_PATH: "${COMFYUI_LOCAL_MODELS_PATH:-~/Documents/ComfyUI/models}"
      deploy: {}
      extra_hosts:
        - "host.docker.internal:host-gateway"
        
    # External ComfyUI service
    external:
      scale: 0
      environment:
        COMFYUI_ENDPOINT: "http://external.comfyui.provider.com"  # Configure external endpoint
      deploy: {}
      extra_hosts: []
    
    # Disabled - no ComfyUI
    disabled:
      scale: 0
      environment:
        COMFYUI_ENDPOINT: "http://comfyui:18188"  # Default for dependent services
        IS_LOCAL_COMFYUI: "false"
      deploy: {}
      extra_hosts: []

  weaviate:
    # Containerized Weaviate
    container:
      scale: 1
      environment:
        QUERY_DEFAULTS_LIMIT: 25
        AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
        PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
        DEFAULT_VECTORIZER_MODULE: 'text2vec-ollama'
        ENABLE_MODULES: 'text2vec-ollama,text2vec-openai,multi2vec-clip,generative-ollama,generative-openai'
        CLUSTER_HOSTNAME: 'weaviate'
        OPENAI_APIKEY: "${OPENAI_API_KEY:-}"
        CLIP_INFERENCE_API: 'http://multi2vec-clip:8080'
        WEAVIATE_URL: "http://weaviate:8080"
        # OLLAMA_ENDPOINT will be set based on ollama source
      deploy: {}
      extra_hosts: []  # Will inherit from ollama if needed
    
    # Local Weaviate running on host machine
    localhost:
      scale: 0
      environment:
        WEAVIATE_URL: "http://host.docker.internal:8080"
      deploy: {}
      extra_hosts:
        - "host.docker.internal:host-gateway"
    
    # Disabled - no vector database
    disabled:
      scale: 0
      environment:
        WEAVIATE_URL: ""
      deploy: {}
      extra_hosts: []

  multi2vec-clip:
    # CPU-only CLIP processing
    container-cpu:
      scale: 1
      environment:
        ENABLE_CUDA: "0"
      deploy: {}
      extra_hosts: []
      
    # GPU-accelerated CLIP processing
    container-gpu:
      scale: 1
      environment:
        ENABLE_CUDA: "1"
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                capabilities: [gpu]
      extra_hosts: []
    
    # Disabled - no CLIP processing
    disabled:
      scale: 0
      environment:
        ENABLE_CUDA: "0"
      deploy: {}
      extra_hosts: []

  stt_provider:
    # GPU-accelerated Parakeet (NVIDIA)
    parakeet-container-gpu:
      scale: 1
      environment:
        PARAKEET_ENDPOINT: "http://parakeet-gpu:8000"
        PARAKEET_DEVICE: "${PARAKEET_GPU_DEVICE:-cuda}"
        PARAKEET_COMPUTE_TYPE: "${PARAKEET_GPU_COMPUTE_TYPE:-float16}"
        PARAKEET_MODEL: "${PARAKEET_MODEL:-nvidia/parakeet-tdt-0.6b-v3}"
        NVIDIA_VISIBLE_DEVICES: "${NVIDIA_VISIBLE_DEVICES:-all}"
        HF_TOKEN: "${HUGGING_FACE_HUB_TOKEN}"
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: 1
                capabilities: [gpu]
      extra_hosts: []

    # Local Parakeet on host (Mac with MLX or Linux with CPU/GPU)
    parakeet-localhost:
      scale: 0
      environment:
        PARAKEET_ENDPOINT: "${PARAKEET_LOCALHOST_URL:-http://host.docker.internal:10300}"
      deploy: {}
      extra_hosts:
        - "host.docker.internal:host-gateway"

    # Disabled
    disabled:
      scale: 0
      environment:
        PARAKEET_ENDPOINT: ""
      deploy: {}
      extra_hosts: []

  # TTS Provider Service (XTTS v2)
  tts_provider:
    # GPU-accelerated XTTS v2 (NVIDIA) via openedai-speech
    xtts-container-gpu:
      scale: 1
      environment:
        XTTS_ENDPOINT: "http://xtts-gpu:8000"
        NVIDIA_VISIBLE_DEVICES: "${NVIDIA_VISIBLE_DEVICES:-all}"
        HF_TOKEN: "${HUGGING_FACE_HUB_TOKEN}"
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: 1
                capabilities: [gpu]
      extra_hosts: []

    # Local XTTS v2 on host (any platform with Python)
    xtts-localhost:
      scale: 0
      environment:
        XTTS_ENDPOINT: "${XTTS_LOCALHOST_URL:-http://host.docker.internal:10400}"
      deploy: {}
      extra_hosts:
        - "host.docker.internal:host-gateway"

    # Disabled
    disabled:
      scale: 0
      environment:
        XTTS_ENDPOINT: ""
      deploy: {}
      extra_hosts: []

  n8n:
    # Containerized n8n
    container:
      scale: 1
      environment:
        PARAKEET_ENDPOINT: "${PARAKEET_ENDPOINT}"
      deploy: {}
      extra_hosts: []
    # Disabled - no n8n
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  searxng:
    # Containerized SearxNG
    container:
      scale: 1
    # Disabled - no search engine
    disabled:
      scale: 0


  neo4j-graph-db:
    # Containerized Neo4j
    container:
      scale: 1
      environment:
        NEO4J_URI: "bolt://neo4j-graph-db:7687"
      deploy: {}
      extra_hosts: []
    
    # Local Neo4j running on host machine
    localhost:
      scale: 0
      environment:
        NEO4J_URI: "bolt://host.docker.internal:7687"
      deploy: {}
      extra_hosts:
        - "host.docker.internal:host-gateway"
    
    # Disabled - no graph database
    disabled:
      scale: 0
      environment:
        NEO4J_URI: ""
      deploy: {}
      extra_hosts: []

  # Supabase Database
  supabase-db:
    # Containerized Supabase DB (only option)
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []

  # Supabase Database Init
  supabase-db-init:
    # Containerized DB initialization
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []
    
    # Disabled - skip initialization
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # Supabase Meta
  supabase-meta:
    # Containerized Meta service
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []
    
    # Disabled - no Meta service
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # Supabase Storage
  supabase-storage:
    # Containerized Storage service
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []
    
    # Disabled - no Storage service
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # Supabase Auth
  supabase-auth:
    # Containerized Auth service
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []
    
    # Disabled - no Auth service
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # Supabase API (PostgREST)
  supabase-api:
    # Containerized API service
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []
    
    # Disabled - no API service
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # Supabase Realtime
  supabase-realtime:
    # Containerized Realtime service
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []
    
    # Disabled - no Realtime service
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # Supabase Studio
  supabase-studio:
    # Containerized Studio service
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []
    
    # Disabled - no Studio service
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # Redis Cache
  redis:
    # Containerized Redis (only option)
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []

  # Kong API Gateway
  kong-api-gateway:
    # Containerized Kong (only option)
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []

  # n8n Init
  n8n-init:
    # Containerized n8n initialization
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []
    
    # Disabled - skip initialization
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # Ollama Model Puller
  ollama-pull:
    # Containerized model puller (auto-managed by llm-provider)
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []
    
    # Disabled - skip model pulling
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # Weaviate Initialization
  weaviate-init:
    # Containerized Weaviate initialization (auto-managed by weaviate)
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []
    
    # Disabled - skip initialization
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # ComfyUI Initialization
  comfyui-init:
    # Containerized ComfyUI initialization (auto-managed by comfyui)
    container:
      scale: 1
      environment: {}
      deploy: {}
      extra_hosts: []
    
    # Disabled - skip initialization
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # Open WebUI
  open-web-ui:
    # Containerized Open WebUI (adaptive service)
    container:
      scale: 1
      environment:
        # Environment variables will be set by adaptive service logic
        OLLAMA_BASE_URL: "${OLLAMA_ENDPOINT}"
        COMFYUI_BASE_URL: "${COMFYUI_ENDPOINT:-http://comfyui:18188}"
        PARAKEET_ENDPOINT: "${PARAKEET_ENDPOINT}"
      deploy: {}
      extra_hosts: []  # Will inherit from llm_provider and comfyui
    
    # Disabled - no Open WebUI
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # Local Deep Researcher
  local-deep-researcher:
    # Containerized Local Deep Researcher (adaptive service)
    container:
      scale: 1
      environment:
        # Environment variables will be set by adaptive service logic
        LLM_PROVIDER_BASE_URL: "${OLLAMA_ENDPOINT}"
        PARAKEET_ENDPOINT: "${PARAKEET_ENDPOINT}"
      deploy: {}
      extra_hosts: []  # Will inherit from llm_provider
    
    # Disabled - no Local Deep Researcher
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

# Services that adapt their configuration based on other services
# These services modify their environment variables based on dependency sources
adaptive_services:
  
  ollama-pull:
    adapts_to: llm_provider
    # Scales to 1 when llm_provider is containerized (ollama-container-cpu or ollama-container-gpu)
    environment_adaptation:
      OLLAMA_HOST_URL: "${OLLAMA_ENDPOINT}"
    extra_hosts_adaptation: "inherit from llm_provider"
    
  comfyui-init:
    adapts_to: comfyui
    # Scales to 1 when comfyui is enabled (handles both local and containerized ComfyUI)
    environment_adaptation:
      COMFYUI_HOST_URL: "${COMFYUI_ENDPOINT}"
    extra_hosts_adaptation: "inherit from comfyui"
    
  backend:
    adapts_to: [llm_provider, weaviate, stt_provider]
    # Always scales to 1 (enabled by default)
    environment_adaptation:
      OLLAMA_BASE_URL: "${OLLAMA_ENDPOINT}"
      WEAVIATE_URL: "http://weaviate:8080"
      PARAKEET_ENDPOINT: "${PARAKEET_ENDPOINT}"
    extra_hosts_adaptation: "inherit from llm_provider"
    
  weaviate-init:
    adapts_to: llm_provider
    # Scales with weaviate (inherits WEAVIATE_SCALE)
    environment_adaptation:
      OLLAMA_ENDPOINT: "${OLLAMA_ENDPOINT}"
    extra_hosts_adaptation: "inherit from llm_provider"

  n8n:
    adapts_to: [stt_provider]
    # Scales to 1 when enabled
    environment_adaptation:
      PARAKEET_ENDPOINT: "${PARAKEET_ENDPOINT}"
    extra_hosts_adaptation: "inherit from stt_provider"

  # JupyterHub Data Science IDE
  jupyterhub:
    adapts_to: [llm_provider, weaviate, neo4j-graph-db, comfyui, stt_provider]
    # Always scales to 0 or 1 based on SOURCE
    environment_adaptation:
      OLLAMA_BASE_URL: "${OLLAMA_ENDPOINT}"
      WEAVIATE_URL: "http://weaviate:8080"
      NEO4J_URI: "bolt://neo4j-graph-db:7687"
      COMFYUI_BASE_URL: "${COMFYUI_ENDPOINT}"
      PARAKEET_ENDPOINT: "${PARAKEET_ENDPOINT}"
    extra_hosts_adaptation: "inherit from llm_provider"



  # Backend API Service
  backend:
    # Containerized Backend API (adaptive service)
    container:
      scale: 1
      environment:
        # Environment variables will be set by adaptive service logic
        OLLAMA_BASE_URL: "${OLLAMA_ENDPOINT}"
        WEAVIATE_URL: "http://weaviate:8080"
        PARAKEET_ENDPOINT: "${PARAKEET_ENDPOINT}"
      deploy: {}
      extra_hosts: []  # Will inherit from llm_provider

    # Disabled - no Backend API
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

  # JupyterHub Data Science IDE
  jupyterhub:
    # Containerized JupyterHub (adaptive service)
    container:
      scale: 1
      environment:
        # Environment variables will be set by adaptive service logic
        JUPYTER_ENABLE_LAB: "yes"
        GRANT_SUDO: "yes"
      deploy: {}
      extra_hosts: []  # Will inherit from llm_provider

    # Disabled - no JupyterHub
    disabled:
      scale: 0
      environment: {}
      deploy: {}
      extra_hosts: []

# Service dependency relationships for proper startup ordering
dependencies:
  data_tier:
    - supabase-db
    - redis
    - neo4j-graph-db
  init_tier:
    - supabase-db-init
    - ollama-pull
    - comfyui-init
    - weaviate-init
    - n8n-init
  core_services:
    - supabase-meta
    - supabase-storage
    - supabase-auth
    - supabase-api
    - supabase-realtime
    - ollama
    - comfyui
    - parakeet
    - weaviate
    - multi2vec-clip
    - searxng
    - n8n
    - n8n-worker
  app_tier:
    - kong-api-gateway
    - supabase-studio
    - local-deep-researcher
    - open-web-ui
    - backend
    - jupyterhub

# Service dependency relationships
# Defines which services require other services to function properly
service_dependencies:
  # n8n requires weaviate for vector operations and AI workflow nodes
  n8n:
    requires: [weaviate]
    optional: [parakeet]
    error_message: "n8n disabled: requires weaviate for vector operations and AI workflow nodes. Enable weaviate (WEAVIATE_SOURCE=container) to use n8n workflows with vector capabilities"
    info_message: "n8n enabled - will connect to available optional services (parakeet)"

  # n8n-worker also requires weaviate (inherits from n8n)
  n8n-worker:
    requires: [weaviate]
    error_message: "n8n-worker disabled: requires weaviate for vector operations"
    
  # Backend has optional dependencies - will connect if available
  backend:
    optional: [neo4j-graph-db, searxng, n8n, weaviate, parakeet]
    info_message: "Backend enabled - will connect to available optional services (neo4j, searxng, n8n, weaviate, parakeet)"
  
  # Open WebUI has optional dependencies - will connect if available
  open-web-ui:
    optional: [weaviate, local-deep-researcher, parakeet]
    info_message: "Open WebUI enabled - will connect to available optional services (weaviate, local-deep-researcher, parakeet)"
  
  # Local Deep Researcher has optional dependencies - will connect if available
  local-deep-researcher:
    optional: [neo4j-graph-db, searxng, n8n, weaviate, parakeet]
    info_message: "Local Deep Researcher enabled - will connect to available optional services (neo4j, searxng, n8n, weaviate, parakeet)"

  # JupyterHub has optional dependencies - will connect if available
  jupyterhub:
    optional: [neo4j-graph-db, searxng, n8n, weaviate, comfyui, parakeet]
    info_message: "JupyterHub enabled - will connect to available optional services (neo4j, searxng, n8n, weaviate, comfyui, parakeet)"