{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Ollama Basics\n",
    "\n",
    "Learn how to use Ollama for LLM inference in the GenAI Vanilla Stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from ollama import Client\n",
    "\n",
    "load_dotenv()\n",
    "client = Client(host=os.getenv(\"OLLAMA_BASE_URL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.list()\n",
    "print(\"Available Models:\")\n",
    "for model in models[\"models\"]:\n",
    "    print(f\"  â€¢ {model['name']} ({model['size'] / 1e9:.1f}GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat(model=\"llama3.2\", messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Explain RAG in one sentence.\"}\n",
    "])\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.chat(model=\"llama3.2\", messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Write a haiku about AI.\"}\n",
    "], stream=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = client.embeddings(model=\"llama3.2\", prompt=\"GenAI Vanilla Stack\")\n",
    "print(f\"Embedding dimension: {len(embedding['embedding'])}\")\n",
    "print(f\"First 5 values: {embedding['embedding'][:5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
