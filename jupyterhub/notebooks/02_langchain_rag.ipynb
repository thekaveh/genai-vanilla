{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - LangChain RAG Pipeline\n",
    "\n",
    "Build a Retrieval-Augmented Generation (RAG) system using Weaviate and Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import weaviate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from ollama import Client\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weaviate\n",
    "weaviate_url = os.getenv(\"WEAVIATE_URL\").replace(\"http://\", \"\")\n",
    "host, port = weaviate_url.split(\":\")\n",
    "wv_client = weaviate.connect_to_custom(\n",
    "    http_host=host,\n",
    "    http_port=int(port),\n",
    "    http_secure=False\n",
    ")\n",
    "\n",
    "# Ollama\n",
    "llm = Ollama(model=\"llama3.2\", base_url=os.getenv(\"OLLAMA_BASE_URL\"))\n",
    "ollama_client = Client(host=os.getenv(\"OLLAMA_BASE_URL\"))\n",
    "\n",
    "print(f\"✅ Connected to Weaviate: {wv_client.is_ready()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Custom Ollama Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaEmbeddings(Embeddings):\n",
    "    def __init__(self, client, model=\"llama3.2\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        return [self.client.embeddings(model=self.model, prompt=text)[\"embedding\"] for text in texts]\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        return self.client.embeddings(model=self.model, prompt=text)[\"embedding\"]\n",
    "\n",
    "embeddings = OllamaEmbeddings(ollama_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"GenAI Vanilla Stack is a modular AI development platform.\",\n",
    "    \"Ollama provides local LLM inference without cloud dependencies.\",\n",
    "    \"Weaviate is a vector database optimized for semantic search.\",\n",
    "    \"JupyterHub enables interactive data science workflows.\",\n",
    "    \"Neo4j stores data as graphs with nodes and relationships.\"\n",
    "]\n",
    "\n",
    "print(f\"Sample documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Embeddings and Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection if not exists\n",
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "\n",
    "if not wv_client.collections.exists(\"Document\"):\n",
    "    wv_client.collections.create(\n",
    "        name=\"Document\",\n",
    "        properties=[Property(name=\"content\", data_type=DataType.TEXT)]\n",
    "    )\n",
    "\n",
    "collection = wv_client.collections.get(\"Document\")\n",
    "\n",
    "# Add documents\n",
    "for doc in documents:\n",
    "    embedding = embeddings.embed_query(doc)\n",
    "    collection.data.insert(properties={\"content\": doc}, vector=embedding)\n",
    "\n",
    "print(\"✅ Documents stored in Weaviate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is a vector database?\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Search\n",
    "results = collection.query.near_vector(near_vector=query_embedding, limit=2)\n",
    "\n",
    "context = \"\\n\".join([obj.properties[\"content\"] for obj in results.objects])\n",
    "\n",
    "# Generate answer\n",
    "prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "answer = llm.invoke(prompt)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nContext: {context}\")\n",
    "print(f\"\\nAnswer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
